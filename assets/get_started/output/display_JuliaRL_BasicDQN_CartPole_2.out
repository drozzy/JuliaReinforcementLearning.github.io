ReinforcementLearningCore.Experiment
├─ policy => ReinforcementLearningCore.Agent
│  ├─ policy => ReinforcementLearningCore.QBasedPolicy
│  │  ├─ learner => ReinforcementLearningZoo.BasicDQNLearner
│  │  │  ├─ approximator => ReinforcementLearningCore.NeuralNetworkApproximator
│  │  │  │  ├─ model => Flux.Chain
│  │  │  │  │  └─ layers
│  │  │  │  │     ├─ 1
│  │  │  │  │     │  └─ Flux.Dense
│  │  │  │  │     │     ├─ W => 128×4 Array{Float32,2}
│  │  │  │  │     │     ├─ b => 128-element Array{Float32,1}
│  │  │  │  │     │     └─ σ => typeof(NNlib.relu)
│  │  │  │  │     ├─ 2
│  │  │  │  │     │  └─ Flux.Dense
│  │  │  │  │     │     ├─ W => 128×128 Array{Float32,2}
│  │  │  │  │     │     ├─ b => 128-element Array{Float32,1}
│  │  │  │  │     │     └─ σ => typeof(NNlib.relu)
│  │  │  │  │     └─ 3
│  │  │  │  │        └─ Flux.Dense
│  │  │  │  │           ├─ W => 2×128 Array{Float32,2}
│  │  │  │  │           ├─ b => 2-element Array{Float32,1}
│  │  │  │  │           └─ σ => typeof(identity)
│  │  │  │  └─ optimizer => Flux.Optimise.ADAM
│  │  │  │     ├─ eta => 0.001
│  │  │  │     ├─ beta
│  │  │  │     │  ├─ 1
│  │  │  │     │  │  └─ 0.9
│  │  │  │     │  └─ 2
│  │  │  │     │     └─ 0.999
│  │  │  │     └─ state => IdDict
│  │  │  ├─ loss_func => typeof(Flux.Losses.huber_loss)
│  │  │  ├─ γ => 0.99
│  │  │  ├─ sampler => ReinforcementLearningCore.BatchSampler
│  │  │  │  └─ batch_size => 32
│  │  │  ├─ min_replay_history => 100
│  │  │  ├─ rng => StableRNGs.LehmerRNG
│  │  │  └─ loss => 0.0
│  │  └─ explorer => ReinforcementLearningCore.EpsilonGreedyExplorer
│  │     ├─ ϵ_stable => 0.01
│  │     ├─ ϵ_init => 1.0
│  │     ├─ warmup_steps => 0
│  │     ├─ decay_steps => 500
│  │     ├─ step => 1
│  │     ├─ rng => StableRNGs.LehmerRNG
│  │     └─ is_training => true
│  └─ trajectory => ReinforcementLearningCore.Trajectory
│     └─ traces => NamedTuple
│        ├─ state => 4×0 CircularArrayBuffers.CircularArrayBuffer{Float32,2}
│        ├─ action => 0-element CircularArrayBuffers.CircularArrayBuffer{Int64,1}
│        ├─ reward => 0-element CircularArrayBuffers.CircularArrayBuffer{Float32,1}
│        └─ terminal => 0-element CircularArrayBuffers.CircularArrayBuffer{Bool,1}
├─ env => ReinforcementLearningEnvironments.CartPoleEnv
├─ stop_condition => ReinforcementLearningCore.StopAfterStep
│  ├─ step => 10000
│  ├─ cur => 1
│  └─ progress => ProgressMeter.Progress
├─ hook => ReinforcementLearningCore.ComposedHook
│  └─ hooks
│     ├─ 1
│     │  └─ ReinforcementLearningCore.TotalRewardPerEpisode
│     │     ├─ rewards => 0-element Array{Float64,1}
│     │     └─ reward => 0.0
│     ├─ 2
│     │  └─ ReinforcementLearningCore.TimePerStep
│     │     ├─ times => 0-element CircularArrayBuffers.CircularArrayBuffer{Float64,1}
│     │     └─ t => 747253550616
│     ├─ 3
│     │  └─ ReinforcementLearningCore.DoEveryNStep
│     │     ├─ f => ReinforcementLearningZoo.var"#334#338"
│     │     ├─ n => 1
│     │     └─ t => 0
│     └─ 4
│        └─ ReinforcementLearningCore.DoEveryNEpisode
│           ├─ f => ReinforcementLearningZoo.var"#336#340"
│           ├─ n => 1
│           └─ t => 0
└─ description => "This experiment uses three dense layers to approximate the Q value...."

