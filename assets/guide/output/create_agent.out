ReinforcementLearningCore.Agent
├─ policy => ReinforcementLearningCore.QBasedPolicy
│  ├─ learner => ReinforcementLearningZoo.BasicDQNLearner
│  │  ├─ approximator => ReinforcementLearningCore.NeuralNetworkApproximator
│  │  │  ├─ model => Flux.Chain
│  │  │  │  └─ layers
│  │  │  │     ├─ 1
│  │  │  │     │  └─ Flux.Dense
│  │  │  │     │     ├─ W => 128×4 Array{Float32,2}
│  │  │  │     │     ├─ b => 128-element Array{Float32,1}
│  │  │  │     │     └─ σ => typeof(NNlib.relu)
│  │  │  │     ├─ 2
│  │  │  │     │  └─ Flux.Dense
│  │  │  │     │     ├─ W => 128×128 Array{Float32,2}
│  │  │  │     │     ├─ b => 128-element Array{Float32,1}
│  │  │  │     │     └─ σ => typeof(NNlib.relu)
│  │  │  │     └─ 3
│  │  │  │        └─ Flux.Dense
│  │  │  │           ├─ W => 2×128 Array{Float32,2}
│  │  │  │           ├─ b => 2-element Array{Float32,1}
│  │  │  │           └─ σ => typeof(identity)
│  │  │  └─ optimizer => Flux.Optimise.ADAM
│  │  │     ├─ eta => 0.001
│  │  │     ├─ beta
│  │  │     │  ├─ 1
│  │  │     │  │  └─ 0.9
│  │  │     │  └─ 2
│  │  │     │     └─ 0.999
│  │  │     └─ state => IdDict
│  │  ├─ loss_func => typeof(ReinforcementLearningCore.huber_loss)
│  │  ├─ γ => 0.99
│  │  ├─ batch_size => 32
│  │  ├─ min_replay_history => 100
│  │  ├─ rng => Random.MersenneTwister
│  │  └─ loss => 0.0
│  └─ explorer => ReinforcementLearningCore.EpsilonGreedyExplorer
│     ├─ ϵ_stable => 0.01
│     ├─ ϵ_init => 1.0
│     ├─ warmup_steps => 0
│     ├─ decay_steps => 500
│     ├─ step => 1
│     ├─ rng => Random.MersenneTwister
│     └─ is_training => true
├─ trajectory => ReinforcementLearningCore.CombinedTrajectory
│  ├─ reward => 0-element ReinforcementLearningCore.CircularArrayBuffer{Float32,1}
│  ├─ terminal => 0-element ReinforcementLearningCore.CircularArrayBuffer{Bool,1}
│  ├─ state => 4×0 view(::ReinforcementLearningCore.CircularArrayBuffer{Float32,2}, :, 1:0) with eltype Float32
│  ├─ next_state => 4×0 view(::ReinforcementLearningCore.CircularArrayBuffer{Float32,2}, :, 2:1) with eltype Float32
│  ├─ full_state => 4×0 view(::ReinforcementLearningCore.CircularArrayBuffer{Float32,2}, :, 1:0) with eltype Float32
│  ├─ action => 0-element view(::ReinforcementLearningCore.CircularArrayBuffer{Int64,1}, 1:0) with eltype Int64
│  ├─ next_action => 0-element view(::ReinforcementLearningCore.CircularArrayBuffer{Int64,1}, 2:1) with eltype Int64
│  └─ full_action => 0-element view(::ReinforcementLearningCore.CircularArrayBuffer{Int64,1}, 1:0) with eltype Int64
├─ role => DEFAULT_PLAYER
└─ is_training => true

