<!doctype html> <html lang=en > <script async src="https://www.googletagmanager.com/gtag/js?id=UA-149861753-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'UA-149861753-1'); </script> <meta charset=utf-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel=icon  href="/assets/site/logo.svg"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <title>How to write a customized environment?</title> <link rel=stylesheet  href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin=anonymous > <link href="/css/custom.css" rel=stylesheet > <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin=anonymous ></script> <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin=anonymous ></script> <script src="/libs/distill/template.v2.8.0.js"></script> <d-front-matter> <script id=distill-front-matter  type="text/json"> { "authors": [ { "author":"Jun Tian", "authorURL":"https://github.com/findmyway", "affiliation":"", "affiliationURL":"" } ], "publishedDate":"2021-01-26T03:29:21.165" } </script> </d-front-matter> <nav class="navbar navbar-expand-lg navbar-dark fixed-top" style="background-color: #1fd1f9; background-image: linear-gradient(315deg, #1fd1f9 0%, #b621fe 74%); " id=mainNav > <div class=container > <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarTogglerDemo01" aria-controls=navbarTogglerDemo01  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarTogglerDemo01 > <span class=navbar-brand > <a class=navbar-brand  href="/"> JuliaReinforcementLearning </a> </span> <ul class="navbar-nav ml-auto"> <li class=nav-item > <a class=nav-link  href="/get_started/">Get Started</a> <li class=nav-item > <a class=nav-link  href="/guide/">Guide</a> <li class=nav-item > <a class=nav-link  href="/contribute/">Contribute</a> <li class=nav-item > <a class=nav-link  href="/blog/">Blog</a> <li class=nav-item > <a class=nav-link  href="https://JuliaReinforcementLearning.github.io/ReinforcementLearning.jl/latest/">Doc</a> <li class=nav-item > <a class=nav-link  href="https://github.com/JuliaReinforcementLearning">Github</a> </ul> </div> </nav> <d-title><h1>How to write a customized environment?</h1><p>The first step to apply algorithms in ReinforcementLearning.jl is to define the problem you want to solve in a recognizable way. Here we&#39;ll demonstrate how to write many different kinds of environments based on interfaces defined in <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/blob/master/src/interface.jl">ReinforcementLearningBase.jl</a>.</p> </d-title> <d-byline></d-byline> <d-article class=franklin-content ><p>The most commonly used interfaces to describe reinforcement learning tasks is <a href="https://gym.openai.com/">OpenAI/Gym</a>. Inspired by it, we expand those interfaces a little to utilize the multiple-dispatch in Julia and to cover multi-agent environments.</p> <h2 id=the_minimal_interfaces_to_implement ><a href="#the_minimal_interfaces_to_implement">The minimal interfaces to implement</a></h2> <p>Many interfaces in <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/blob/master/src/interface.jl">ReinforcementLearningBase.jl</a> have a default implementation. So in most cases, you only need to implement the following functions to define a customized environment:</p> <pre><code class="julia hljs">get_actions(env::YourEnv)
get_state(env::YourEnv)
get_reward(env::YourEnv)
get_terminal(env::YourEnv)
reset!(env::YourEnv)
(env::YourEnv)(action)</code></pre> <p>Here we use an example introduced in <a href="https://www.informs-sim.org/wsc18papers/includes/files/021.pdf">Monte Carlo Tree Search: A Tutorial</a> to demonstrate how to write a simple environment.</p> <p>The game is defined like this: assume you have &#36;10 in your pocket, and you are faced with the following three choices:</p> <ol> <li><p>Buy a PowerRich lottery ticket &#40;win &#36;100M w.p. 0.01; nothing otherwise&#41;;</p> <li><p>Buy a MegaHaul lottery ticket &#40;win &#36;1M w.p. 0.05; nothing otherwise&#41;;</p> <li><p>Do not buy a lottery ticket.</p> </ol> <p>First we define a concrete subtype of <code>AbstractEnv</code> named <code>LotteryEnv</code>:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> ReinforcementLearningBase

<span class=hljs-keyword >mutable struct</span> LotteryEnv &lt;: AbstractEnv
    reward::<span class=hljs-built_in >Union</span>{<span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Int</span>}
<span class=hljs-keyword >end</span>

LotteryEnv() = LotteryEnv(<span class=hljs-literal >nothing</span>)</code></pre> <p><code>LotteryEnv</code> has only one field named <code>reward</code>, by default it is initialized with <code>nothing</code>. Now let&#39;s implement the necessary interfaces:</p> <pre><code class="julia hljs">RLBase.get_actions(env::LotteryEnv) = (:PowerRich, :MegaHaul, <span class=hljs-literal >nothing</span>)</code></pre>
<p>Here <code>RLBase</code> is just an alias for <code>ReinforcementLearningBase</code>.</p>
<pre><code class="julia hljs">RLBase.get_reward(env::LotteryEnv) = env.reward
RLBase.get_state(env::LotteryEnv) = !isnothing(env.reward)
RLBase.get_terminal(env::LotteryEnv) = !isnothing(env.reward)
RLBase.reset!(env::LotteryEnv) = env.reward = <span class=hljs-literal >nothing</span></code></pre>
<p>Because the lottery game is just a simple one-shot game. If the <code>reward</code> is <code>nothing</code> then the game is not terminated yet and we say the game is in state <code>false</code>, otherwise the game is terminated and the state is <code>true</code>. By <code>reset&#33;</code> the game, we simply assign the reward with <code>nothing</code>, meaning that it&#39;s in the initial state.</p>
<p>The only left one is to implement the game logic:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> (env::LotteryEnv)(action)
    <span class=hljs-keyword >if</span> action == :PowerRich
        env.reward = rand() &lt; <span class=hljs-number >0.01</span> ? <span class=hljs-number >100_000_000</span> : -<span class=hljs-number >10</span>
    <span class=hljs-keyword >elseif</span> action == :MegaHaul
        env.reward = rand() &lt; <span class=hljs-number >0.05</span> ? <span class=hljs-number >1_000_000</span> : -<span class=hljs-number >10</span>
    <span class=hljs-keyword >else</span>
        env.reward = <span class=hljs-number >0</span>
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre>
<p>A simple way to check that your environment works is to apply the <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_base/#ReinforcementLearningBase.RandomPolicy"><code>RandomPolicy</code></a> to the environment.</p>
<pre><code class="julia hljs">env = LotteryEnv()
run(RandomPolicy(env), env)</code></pre>
<p>One step further is to test that other components in ReinforcementLearning.jl also work:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> ReinforcementLearning
hook = TotalRewardPerEpisode()
run(
    Agent(
        ;policy = RandomPolicy(env),
        trajectory = VectCompactSARTSATrajectory(
            state_type=<span class=hljs-built_in >Bool</span>,
            action_type=<span class=hljs-built_in >Any</span>,
            reward_type=<span class=hljs-built_in >Int</span>,
            terminal_type=<span class=hljs-built_in >Bool</span>,
        ),
    ),
    LotteryEnv(),
    StopAfterEpisode(<span class=hljs-number >1_000</span>),
    hook
)

println(sum(hook.rewards) / <span class=hljs-number >1_000</span>)</code></pre>
<pre><code class="plaintext hljs">UndefVarError: VectCompactSARTSATrajectory not defined
</code></pre>
<h2 id=traits_of_environments ><a href="#traits_of_environments">Traits of environments</a></h2>
<p>If you run <code>LotteryEnv&#40;&#41;</code> in the REPL, you&#39;ll get the following summary of the environment:</p>

<pre><code class="plaintext hljs">method not implemented
</code></pre>
<p>The <strong>Traits</strong> section describes which category the environment belongs to. As you can see, by default an environment is assumed to be of:</p>
<ul>
<li><p><code>SingleAgent</code></p>

<li><p><code>Sequential</code></p>

<li><p><code>PerfectInformation</code></p>

<li><p><code>Deterministic</code></p>

<li><p><code>StepReward</code></p>

<li><p><code>GeneralSum</code></p>

<li><p><code>MinimalActionSet</code></p>

</ul>
<h3 id=actionstyle ><a href="#actionstyle">ActionStyle</a></h3>

<pre><code class="julia hljs">ActionStyle(env::AbstractEnv)</code></pre></p>
<p>For environments of discrete actions, specify whether the current state of <code>env</code> contains a full action set or a minimal action set. By default the <a href="@ref"><code>MINIMAL_ACTION_SET</code></a> is returned.
<p>For environments of <code>FULL_ACTION_SET</code>, the following methods must be implemented:</p>
<ul>
<li><p><code>get_legal_actions&#40;env&#41;</code></p>

<li><p><code>get_legal_actions_mask&#40;env&#41;</code></p>

</ul>
<h3 id=dynamicstyle ><a href="#dynamicstyle">DynamicStyle</a></h3>

<pre><code class="julia hljs">DynamicStyle(env::AbstractEnv) = SEQUENTIAL</code></pre>
<p>Only valid in environments with a <a href="@ref"><code>NumAgentStyle</code></a> of <a href="@ref"><code>MultiAgent</code></a>. Determine whether the players can play simultaneously or not. Possible returns are:</p>
<ul>
<li><p><a href="@ref"><code>SEQUENTIAL</code></a>. This is the default return.</p>

<li><p><a href="@ref"><code>SIMULTANEOUS</code></a>.</p>

</ul>

<p>For environment of <code>SIMULTANEOUS</code>, the actions in each step must be a collection, representing the joint actions from all players.</p>
<h3 id=utilitystyle ><a href="#utilitystyle">UtilityStyle</a></h3>

<pre><code class="julia hljs">UtilityStyle(env::AbstractEnv)</code></pre>
<p>Specify the utility style in multi-agent environments. Possible values are:</p>
<ul>
<li><p><a href="@ref">GENERAL_SUM</a>. The default return.</p>

<li><p><a href="@ref">ZERO_SUM</a></p>

<li><p><a href="@ref">CONSTANT_SUM</a></p>

<li><p><a href="@ref">IDENTICAL_UTILITY</a></p>

</ul>

<h3 id=rewardstyle ><a href="#rewardstyle">RewardStyle</a></h3>

<p>Specify whether we can get reward after each step or only at the end of an game. Possible values are <a href="@ref"><code>STEP_REWARD</code></a> &#40;the default one&#41; or <a href="@ref"><code>TERMINAL_REWARD</code></a>.</p>
<div class="admonition note"><p class=admonition-title >Note</p><p>Environments of <a href="@ref"><code>TERMINAL_REWARD</code></a> style can be viewed as a subset of environments of <a href="@ref"><code>STEP_REWARD</code></a> style. For some algorithms, like MCTS, we may have some a more efficient implementation for environments of <a href="@ref"><code>TERMINAL_REWARD</code></a> style.</p>
</div>

<p>Some algorithms may use this trait for acceleration.</p>
<h3 id=chancestyle ><a href="#chancestyle">ChanceStyle</a></h3>

<pre><code class="julia hljs">ChanceStyle(env) = DETERMINISTIC</code></pre>
<p>Specify which role the chance plays in the <code>env</code>. Possible returns are:</p>
<ul>
<li><p><a href="@ref"><code>STOCHASTIC</code></a>. This is the default return.</p>

<li><p><a href="@ref"><code>DETERMINISTIC</code></a></p>

<li><p><a href="@ref"><code>EXPLICIT_STOCHASTIC</code></a></p>

<li><p><a href="@ref"><code>SAMPLED_STOCHASTIC</code></a></p>

</ul>

<p>Possible values are:</p>
<ul>
<li><p><code>Deterministic</code></p>

<li><p><code>Stochastic</code></p>

<li><p><code>ExplicitStochastic</code></p>

<li><p><code>SampledStochastic</code></p>

</ul>
<p>Some algorithms may only work on environments of <code>Deterministic</code> or <code>ExplicitStochastic</code>.</p>
<h3 id=informationstyle ><a href="#informationstyle">InformationStyle</a></h3>

<pre><code class="julia hljs">InformationStyle(env) = IMPERFECT_INFORMATION</code></pre></p>
<p>Distinguish environments between <a href="@ref"><code>PERFECT_INFORMATION</code></a> and <a href="@ref"><code>IMPERFECT_INFORMATION</code></a>. <a href="@ref"><code>IMPERFECT_INFORMATION</code></a> is returned by default.
<h3 id=numagentstyle ><a href="#numagentstyle">NumAgentStyle</a></h3>

<pre><code class="julia hljs">NumAgentStyle(env)</code></pre>
<p>Number of agents involved in the <code>env</code>. Possible returns are:</p>
<ul>
<li><p><a href="@ref"><code>SINGLE_AGENT</code></a>. This is the default return.</p>

<li><p>&#91;<code>MultiAgent</code>&#93;&#91;@ref&#93;.</p>

</ul>

<p>The <code>NumAgentStyle</code> trait is used to define the number of agents in an environment. Possible values are <code>SINGLE_AGENT</code> or <code>MultiAgent&#123;N&#125;&#40;&#41;</code>. In multi-agent environments, a special case is <code>Two_Agent</code>, which is an alias of <code>MultiAgent&#123;2&#125;&#40;&#41;</code>. For multi-agent environments, many functions need to accept another argument named <code>player</code> &#40;for example <code>get_reward&#40;env,player&#41;</code>&#41; to support getting information from the perspective of a specific player. Here&#39;s the list of these functions:</p>

<pre><code class="plaintext hljs">action_space
legal_action_space
legal_action_space_mask
state
state_space
reward
</code></pre>
<h2 id=environment_wrappers ><a href="#environment_wrappers">Environment wrappers</a></h2>
<p>Some useful environment wrappers are also provided in <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/blob/master/src/interface.jl">ReinforcementLearningBase.jl</a> to mimic OOP. For example, in the above <code>LotteryEnv</code>, actions are of type <code>Union&#123;Symbol, Nothing&#125;</code>. Some algorithms may require that the actions must be discrete integers. Then we can create a wrapped environment:</p>
<pre><code class="julia hljs">inner_env = LotteryEnv()
env = inner_env |&gt; ActionTransformedEnv(a -&gt; get_actions(inner_env)[a])
RLBase.get_actions(env::ActionTransformedEnv{&lt;:LotteryEnv}) = <span class=hljs-number >1</span>:<span class=hljs-number >3</span></code></pre>
<p>In some other cases, we may want to transform the state into integers. Similarly we can achieve this goal with the following code:</p>
<pre><code class="julia hljs">env = LotteryEnv() |&gt; StateOverriddenEnv(s -&gt; <span class=hljs-built_in >Int</span>(s))</code></pre>
<p>See the full list of other environment wrappers <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/blob/master/src/implementations/environments.jl">here</a>.</p>


<div></div></d-article>
          

    
    
        


    

    <d-appendix>
    
    <d-bibliography src="/assets/blog/how_to_write_a_customized_environment/code/bibliography.bib"></d-bibliography>
</d-appendix>

    <div class="distill-site-nav distill-site-footer">
      <div class=row >
        <div class=col-md-3 ></div>
        <div class=col-md-6 >
          <p>This website is built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> of the <a href="https://github.com/tlienart/DistillTemplate">DistillTemplate</a> (licensed under <a href="https://github.com/distillpub/template/blob/master/LICENSE">Apache License 2.0</a>) and <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a>. The <a href="https://github.com/JuliaReinforcementLearning/JuliaReinforcementLearning.github.io">source code</a> of this website is licensed under <a href="https://github.com/JuliaReinforcementLearning/JuliaReinforcementLearning.github.io/blob/master/LICENSE">MIT License</a>. The <a href="https://github.com/JuliaReinforcementLearning">JuliaReinforcementLearning</a> organization was first created by <a href="https://github.com/jbrea">Johanni Brea</a> and then co-maintained by <a href="https://github.com/findmyway">Jun Tian</a>. And we thank all the contributors <sup>[<a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/graphs/contributors">1</a>, <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/graphs/contributors">2</a>, <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningCore.jl/graphs/contributors">3</a>, <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl/graphs/contributors">4</a>, <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/graphs/contributors">5</a>, <a href="https://github.com/JuliaReinforcementLearning/ArcadeLearningEnvironment.jl/graphs/contributors">6</a>]</sup> in this organization.</p>
        </div>
        <div class=col-md-3 ></div>
      </div>
    </div>