<!doctype html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-149861753-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-149861753-1');
</script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="icon" href="/assets/site/logo.svg">

  
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
   

  <title>How to write a customized environment?</title>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css"
      integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
  <link href="/css/custom.css" rel="stylesheet">

  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>

  <!-- distill -->
  <script src="/libs/distill/template.v2.8.0.js"></script>
  <d-front-matter>
    <script id="distill-front-matter" type="text/json">
        {
    "authors": [
        {
            "author":"Jun Tian",
            "authorURL":"https://github.com/findmyway",
            "affiliation":"",
            "affiliationURL":""
        }
    ],
    "publishedDate":"2020-08-06T14:41:36.438"
}
    </script>
</d-front-matter>

</head>
<body>
  <nav class="navbar navbar-expand-lg  navbar-dark fixed-top" style="background-color: #1fd1f9; background-image: linear-gradient(315deg, #1fd1f9 0%, #b621fe 74%); " id="mainNav">
  <div class="container">
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarTogglerDemo01" aria-controls="navbarTogglerDemo01" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarTogglerDemo01">
      <span class="navbar-brand">
          <a class="navbar-brand" href="/">
            <!-- <img src="/assets/site/logo.svg" width="30" height="30" alt="logo" loading="lazy"> -->
            JuliaReinforcementLearning
          </a>
      </span>

      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link" href="/get_started/">Get Started</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/guide/">Guide</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/contribute/">Contribute</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/blog/">Blog</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://JuliaReinforcementLearning.github.io/ReinforcementLearning.jl/latest/">Doc</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/JuliaReinforcementLearning">Github</a>
        </li>
      </ul>
    </div>
</nav>

    <d-title><h1>How to write a customized environment?</h1><p>The first step to apply algorithms in ReinforcementLearning.jl is to define the problem you want to solve in a recognizable way. Here we&#39;ll demonstrate how to write many different kinds of environments based on interfaces defined in <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/blob/master/src/interface.jl">ReinforcementLearningBase.jl</a>.</p>
</d-title>
    <d-byline></d-byline>
    
<!-- Content appended here -->
<d-article class="franklin-content"><p>The most commonly used interfaces to describe reinforcement learning tasks is <a href="https://gym.openai.com/">OpenAI/Gym</a>. Inspired by it, we expand those interfaces a little to utilize the multiple-dispatch in Julia and to cover multi-agent environments.</p>
<h2 id="the_minimal_interfaces_to_implement"><a href="#the_minimal_interfaces_to_implement">The minimal interfaces to implement</a></h2>
<p>Many interfaces in <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/blob/master/src/interface.jl">ReinforcementLearningBase.jl</a> have a default implementation. So in most cases, you only need to implement the following functions to define a customized environment:</p>
<pre><code class="julia hljs">get_actions(env::YourEnv)
get_state(env::YourEnv)
get_reward(env::YourEnv)
get_terminal(env::YourEnv)
reset!(env::YourEnv)
(env::YourEnv)(action)</code></pre>
<p>Here we use an example introduced in <a href="https://www.informs-sim.org/wsc18papers/includes/files/021.pdf">Monte Carlo Tree Search: A Tutorial</a> to demonstrate how to write a simple environment.</p>
<p>The game is defined like this: assume you have &#36;10 in your pocket, and you are faced with the following three choices:</p>
<ol>
<li><p>Buy a PowerRich lottery ticket &#40;win &#36;100M w.p. 0.01; nothing otherwise&#41;;</p>
</li>
<li><p>Buy a MegaHaul lottery ticket &#40;win &#36;1M w.p. 0.05; nothing otherwise&#41;;</p>
</li>
<li><p>Do not buy a lottery ticket.</p>
</li>
</ol>
<p>First we define a concrete subtype of <code>AbstractEnv</code> named <code>LotteryEnv</code>:</p>
<pre><code class="julia hljs"><span class="hljs-keyword">using</span> ReinforcementLearningBase

<span class="hljs-keyword">mutable struct</span> LotteryEnv &lt;: AbstractEnv
    reward::<span class="hljs-built_in">Union</span>{Nothing, <span class="hljs-built_in">Int</span>}
<span class="hljs-keyword">end</span>

LotteryEnv() = LotteryEnv(<span class="hljs-literal">nothing</span>)</code></pre>
<p><code>LotteryEnv</code> has only one field named <code>reward</code>, by default it is initialized with <code>nothing</code>. Now let&#39;s implement the necessary interfaces:</p>
<pre><code class="julia hljs">RLBase.get_actions(env::LotteryEnv) = (:PowerRich, :MegaHaul, <span class="hljs-literal">nothing</span>)</code></pre>
<p>Here <code>RLBase</code> is just an alias for <code>ReinforcementLearningBase</code>.</p>
<pre><code class="julia hljs">RLBase.get_reward(env::LotteryEnv) = env.reward
RLBase.get_state(env::LotteryEnv) = !isnothing(env.reward)
RLBase.get_terminal(env::LotteryEnv) = !isnothing(env.reward)
RLBase.reset!(env::LotteryEnv) = env.reward = <span class="hljs-literal">nothing</span></code></pre>
<p>Because the lottery game is just a simple one-shot game. If the <code>reward</code> is <code>nothing</code> then the game is not terminated yet and we say the game is in state <code>false</code>, otherwise the game is terminated and the state is <code>true</code>. By <code>reset&#33;</code> the game, we simply assign the reward with <code>nothing</code>, meaning that it&#39;s in the initial state.</p>
<p>The only left one is to implement the game logic:</p>
<pre><code class="julia hljs"><span class="hljs-keyword">function</span> (env::LotteryEnv)(action)
    <span class="hljs-keyword">if</span> action == :PowerRich
        env.reward = rand() &lt; <span class="hljs-number">0.01</span> ? <span class="hljs-number">100_000_000</span> : -<span class="hljs-number">10</span>
    <span class="hljs-keyword">elseif</span> action == :MegaHaul
        env.reward = rand() &lt; <span class="hljs-number">0.05</span> ? <span class="hljs-number">1_000_000</span> : -<span class="hljs-number">10</span>
    <span class="hljs-keyword">else</span>
        env.reward = <span class="hljs-number">0</span>
    <span class="hljs-keyword">end</span>
<span class="hljs-keyword">end</span></code></pre>
<p>A simple way to check that your environment works is to apply the <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_base/#ReinforcementLearningBase.RandomPolicy"><code>RandomPolicy</code></a> to the environment.</p>
<pre><code class="julia hljs">env = LotteryEnv()
run(RandomPolicy(env), env)</code></pre>
<p>One step further is to test that other components in ReinforcementLearning.jl also work:</p>
<pre><code class="julia hljs"><span class="hljs-keyword">using</span> ReinforcementLearning
hook = TotalRewardPerEpisode()
run(
    Agent(
        ;policy = RandomPolicy(env),
        trajectory = VectorialCompactSARTSATrajectory(
            state_type=<span class="hljs-built_in">Bool</span>,
            action_type=<span class="hljs-built_in">Any</span>,
            reward_type=<span class="hljs-built_in">Int</span>,
            terminal_type=<span class="hljs-built_in">Bool</span>,
        ),
    ),
    LotteryEnv(),
    StopAfterEpisode(<span class="hljs-number">1_000</span>),
    hook
)

println(sum(hook.rewards) / <span class="hljs-number">1_000</span>)</code></pre>
<pre><code class="plaintext hljs">UndefVarError: env not defined
</code></pre>
<h2 id="traits_of_environments"><a href="#traits_of_environments">Traits of environments</a></h2>
<p>If you run <code>LotteryEnv&#40;&#41;</code> in the REPL, you&#39;ll get the following summary of the environment:</p>

<pre><code class="plaintext hljs">UndefVarError: LotteryEnv not defined
</code></pre>
<p>The <strong>Traits</strong> section describes which category the environment belongs to. As you can see, by default an environment is assumed to be of:</p>
<ul>
<li><p><code>SingleAgent</code></p>
</li>
<li><p><code>Sequential</code></p>
</li>
<li><p><code>PerfectInformation</code></p>
</li>
<li><p><code>Deterministic</code></p>
</li>
<li><p><code>StepReward</code></p>
</li>
<li><p><code>GeneralSum</code></p>
</li>
<li><p><code>MinimalActionSet</code></p>
</li>
</ul>
<h3 id="actionstyle"><a href="#actionstyle">ActionStyle</a></h3>

<pre><code class="julia hljs">ActionStyle(env::AbstractEnv)</code></pre></p>
<p>Specify whether the current state of <code>env</code> contains a full action set or a minimal action set. By default the <a href="@ref"><code>MINIMAL_ACTION_SET</code></a> is returned.
<p>For environments of <code>FULL_ACTION_SET</code>, the following methods must be implemented:</p>
<ul>
<li><p><code>get_legal_actions&#40;env&#41;</code></p>
</li>
<li><p><code>get_legal_actions_mask&#40;env&#41;</code></p>
</li>
</ul>
<h3 id="dynamicstyle"><a href="#dynamicstyle">DynamicStyle</a></h3>

<pre><code class="julia hljs">DynamicStyle(env::AbstractEnv) = SEQUENTIAL</code></pre></p>
<p>Determine whether the players can play simultaneously or not. Default value is <a href="@ref"><code>SEQUENTIAL</code></a>
<p>For environment of <code>SIMULTANEOUS</code>, the actions in each step must be a collection, representing the joint actions from all players.</p>
<h3 id="utilitystyle"><a href="#utilitystyle">UtilityStyle</a></h3>

<pre><code class="julia hljs">UtilityStyle(env::AbstractEnv)</code></pre>
<p>Specify the utility style in multi-agent environments. Possible values are:</p>
<ul>
<li><p><a href="@ref">ZERO_SUM</a></p>
</li>
<li><p><a href="@ref">CONSTANT_SUM</a></p>
</li>
<li><p><a href="@ref">GENERAL_SUM</a></p>
</li>
<li><p><a href="@ref">IDENTICAL_REWARD</a></p>
</li>
</ul>

<h3 id="rewardstyle"><a href="#rewardstyle">RewardStyle</a></h3>

Specify whether we can get reward after each step or only at the end of an game. Possible values are <a href="@ref">STEP_REWARD</a> or <a href="@ref">TERMINAL_REWARD</a>
<p>Some algorithms may use this trait for acceleration.</p>
<h3 id="chancestyle"><a href="#chancestyle">ChanceStyle</a></h3>

<pre><code class="julia hljs">ChanceStyle(env) = DETERMINISTIC</code></pre>
<p>Possible values are:</p>
<ul>
<li><p><code>Deterministic</code></p>
</li>
<li><p><code>Stochastic</code></p>
</li>
<li><p><code>ExplicitStochastic</code></p>
</li>
<li><p><code>SampledStochastic</code></p>
</li>
</ul>
<p>Some algorithms may only work on environments of <code>Deterministic</code> or <code>ExplicitStochastic</code>.</p>
<h3 id="informationstyle"><a href="#informationstyle">InformationStyle</a></h3>

<pre><code class="julia hljs">InformationStyle(env) = PERFECT_INFORMATION</code></pre></p>
<p>Specify whether the <code>env</code> is <a href="@ref">PERFECT_INFORMATION</a> or <a href="@ref">IMPERFECT_INFORMATION</a>. Return <a href="@ref">PERFECT_INFORMATION</a> by default.
<h3 id="numagentstyle"><a href="#numagentstyle">NumAgentStyle</a></h3>

<pre><code class="julia hljs">NumAgentStyle(env)</code></pre>
<p>The <code>NumAgentStyle</code> trait is used to define the number of agents in an environment. Possible values are <code>SINGLE_AGENT</code> or <code>MultiAgent&#123;N&#125;&#40;&#41;</code>. In multi-agent environments, a special case is <code>Two_Agent</code>, which is an alias of <code>MultiAgent&#123;2&#125;&#40;&#41;</code>. For multi-agent environments, many functions need to accept another argument named <code>player</code> &#40;for example <code>get_reward&#40;env,player&#41;</code>&#41; to support getting information from the perspective of a specific player. Here&#39;s the list of these functions:</p>

<pre><code class="plaintext hljs">get_actions
get_legal_actions
get_legal_actions_mask
get_state
get_history
get_terminal
get_reward
get_prob
</code></pre>
<h2 id="environment_wrappers"><a href="#environment_wrappers">Environment wrappers</a></h2>
<p>Some useful environment wrappers are also provided in <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/blob/master/src/interface.jl">ReinforcementLearningBase.jl</a> to mimic OOP. For example, in the above <code>LotteryEnv</code>, actions are of type <code>Union&#123;Symbol, Nothing&#125;</code>. Some algorithms may require that the actions must be discrete integers. Then we can create a wrapped environment:</p>
<pre><code class="julia hljs">inner_env = LotteryEnv()
env = inner_env |&gt; ActionTransformedEnv(a -&gt; get_actions(inner_env)[a])
RLBase.get_actions(env::ActionTransformedEnv{&lt;:LotteryEnv}) = <span class="hljs-number">1</span>:<span class="hljs-number">3</span></code></pre>
<p>In some other cases, we may want to transform the state into integers. Similarly we can achieve this goal with the following code:</p>
<pre><code class="julia hljs">env = LotteryEnv() |&gt; StateOverriddenEnv(s -&gt; <span class="hljs-built_in">Int</span>(s))</code></pre>
<p>See the full list of other environment wrappers <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/blob/master/src/implementations/environments.jl">here</a>.</p>

<!-- this is necessary!!! not sure why... -->
<div></div></d-article><!-- CONTENT ENDS HERE -->
          <!-- <d-bibliography src="bibliography.bib"></d-bibliography> -->

    
    
        


    

    <d-appendix>
    
    <d-bibliography src="/assets/blog/how_to_write_a_customized_environment/code/bibliography.bib"></d-bibliography>
</d-appendix>

    <div class="distill-site-nav distill-site-footer">
      <div class="row">
        <div class="col-md-3"></div>
        <div class="col-md-6">
          <p>This website is built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> of the <a href="https://github.com/tlienart/DistillTemplate">DistillTemplate</a> (licensed under <a href="https://github.com/distillpub/template/blob/master/LICENSE">Apache License 2.0</a>) and <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a>. The <a href="https://github.com/JuliaReinforcementLearning/JuliaReinforcementLearning.github.io">source code</a> of this website is licensed under <a href="https://github.com/JuliaReinforcementLearning/JuliaReinforcementLearning.github.io/blob/master/LICENSE">MIT License</a>. The <a href="https://github.com/JuliaReinforcementLearning">JuliaReinforcementLearning</a> organization was first created by <a href="https://github.com/jbrea">Johanni Brea</a> and then co-maintained by <a href="https://github.com/findmyway">Jun Tian</a>. And we thank all the contributors <sup>[<a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/graphs/contributors">1</a>, <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/graphs/contributors">2</a>, <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningCore.jl/graphs/contributors">3</a>, <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl/graphs/contributors">4</a>, <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/graphs/contributors">5</a>, <a href="https://github.com/JuliaReinforcementLearning/ArcadeLearningEnvironment.jl/graphs/contributors">6</a>]</sup> in this organization.</p>
        </div>
        <div class="col-md-3"></div>
      </div>
    </div>
  </body>
</html>
