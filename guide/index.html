<!doctype html> <html lang=en > <script async src="https://www.googletagmanager.com/gtag/js?id=UA-149861753-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'UA-149861753-1'); </script> <meta charset=utf-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel=icon  href="/assets/site/logo.svg"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <title>A Beginner's Guide to ReinforcementLearning.jl</title> <link rel=stylesheet  href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin=anonymous > <link href="/css/custom.css" rel=stylesheet > <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin=anonymous ></script> <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin=anonymous ></script> <script src="/libs/distill/template.v2.8.0.js"></script> <d-front-matter> <script id=distill-front-matter  type="text/json"> { "authors": [ { "author":"Jun Tian", "authorURL":"https://github.com/findmyway", "affiliation":"", "affiliationURL":"" } ], "publishedDate":"2021-01-30T08:13:32.908", "citationText":"Jun Tian, 2021" } </script> </d-front-matter> <nav class="navbar navbar-expand-lg navbar-dark fixed-top" style="background-color: #1fd1f9; background-image: linear-gradient(315deg, #1fd1f9 0%, #b621fe 74%); " id=mainNav > <div class=container > <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarTogglerDemo01" aria-controls=navbarTogglerDemo01  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarTogglerDemo01 > <span class=navbar-brand > <a class=navbar-brand  href="/"> JuliaReinforcementLearning </a> </span> <ul class="navbar-nav ml-auto"> <li class=nav-item > <a class=nav-link  href="/get_started/">Get Started</a> <li class=nav-item > <a class=nav-link  href="/guide/">Guide</a> <li class=nav-item > <a class=nav-link  href="/contribute/">Contribute</a> <li class=nav-item > <a class=nav-link  href="/blog/">Blog</a> <li class=nav-item > <a class=nav-link  href="https://JuliaReinforcementLearning.github.io/ReinforcementLearning.jl/latest/">Doc</a> <li class=nav-item > <a class=nav-link  href="https://github.com/JuliaReinforcementLearning">Github</a> </ul> </div> </nav> <d-title><h1>A Beginner's Guide to ReinforcementLearning.jl</h1><p>From Novice to Professional</p> </d-title> <d-byline></d-byline> <hr class=franklin-toc-separator > <d-article class=franklin-content > <h3 class=franklin-toc-header >Table of content</h3> <div class=franklin-toc ><ol><li><a href="#what_is_an_agent">What is an agent?</a><ol><li><a href="#policy">Policy</a><li><a href="#trajectory">Trajectory</a><li><a href="#a_concrete_example">A concrete example</a></ol><li><a href="#how_to_write_a_customized_environment">How to write a customized environment?</a><li><a href="#how_to_write_a_customized_stop_condition">How to write a customized stop condition?</a><li><a href="#how_to_write_a_customized_hook">How to write a customized hook?</a><li><a href="#how_to_use_tensorboard">How to use TensorBoard?</a><li><a href="#how_to_evaluate_an_agent_during_training">How to evaluate an agent during training?</a></ol></div> </d-article> <hr class=franklin-toc-separator > <d-article class=franklin-content ><p>Here we collect some common questions and answers to help you gain a better understanding of ReinforcementLearning.jl. After trying some prebuilt examples, people are usually interested the following questions:</p> <ul> <li><p>How to understand the XXX algorithm implemented in this package?</p> <li><p>How to apply the XXX algorithm to my problem?</p> <li><p>How to write a new algorithm by reusing components in this package as much as possible?</p> </ul> <p>Now let&#39;s discuss them one by one. First, we&#39;ll focus on the simplest one: DQN<d-cite key=mnih2013playing ></d-cite>. Before looking into details, let&#39;s answer some general questions first.</p> <h2 id=what_is_an_agent ><a href="#what_is_an_agent">What is an agent?</a></h2> <p><em>&quot;So what is the definition of agent in this package?&quot;</em></p> <p>As we have said in the <a href="/get_started#agent">Get Started</a> section:</p> <blockquote> <p>agent is a functional object which takes in an environment and returns an action</p> </blockquote> <p><em>&quot;Nonono, I mean how to understand the Agent data structure used in this package.&quot;</em></p> <p>Well, though it&#39;s not easy to fully understand all the functionalities for the first time, an agent simply contains two parts:</p> <ul> <li><p><strong>Policy</strong></p> <li><p><strong>Trajectory</strong></p> </ul> <h3 id=policy ><a href="#policy">Policy</a></h3> <p>Similar to agent, a policy is also <em>a functional object which takes in an environment and returns an action</em>.</p> <aside>In Reinforcement Learning, people usually like to use the character <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span> to represent policy.</aside> <p><em>&quot;What? So why not just call it agent?&quot;</em></p> <p>Because in our design, policy is a more low-level concept compared to agent. You can think of agent as an orchestrator. It passes states and actions between inner policy and environment. In the meanwhile, an agent will record some useful data into <strong>trajectory</strong> <d-footnote>People usually call it experience replay buffer. However, we choose the name of <em>trajectory</em> here because it is more general. It can be used to store not only experiences but also some intermediate data.</d-footnote> and generate training data to update the inner policy at appropriate time.</p> <p>The simplest policy is <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_base/#ReinforcementLearningBase.RandomPolicy"><code>RandomPolicy</code></a>. It doesn&#39;t need to be updated at all.</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> ReinforcementLearning
env = CartPoleEnv()
p = RandomPolicy(env)
a = p(env)</code></pre> <p>Another more common policy is <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_core/#ReinforcementLearningCore.QBasedPolicy"><code>QBasedPolicy</code></a>. It first maps a state into action values via a Q <code>learner</code>, then an <code>explorer</code> is applied to get the final action. One of the most common explorer is <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_core/#Explorers-1"><code>EpsilonGreedyExplorer</code></a>. For a full list of available explorers, please visit the <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_core/#Explorers-1">doc</a>.</p> <figure class="l-body text-center"> <img src="/guide/q_based_policy.png"> <figcaption>A visual explanation of <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_core/#ReinforcementLearningCore.QBasedPolicy"><code>QBasedPolicy</code></a> in a maze environment.</figcaption> </figure> <h3 id=trajectory ><a href="#trajectory">Trajectory</a></h3> <p>As we mentioned above, a trajectory is used to store some useful data during interactions between policy and environment. Different trajectories have different implementations internally for efficiency or some specific scenario. But they all have the following methods implemented:</p> <pre><code class="julia hljs">t = CircularCompactSARTSATrajectory(;capacity=<span class=hljs-number >3</span>)
push!(t; state=<span class=hljs-number >1</span>, action=<span class=hljs-number >1</span>, reward=<span class=hljs-number >0.</span>, terminal=<span class=hljs-literal >false</span>, next_state=<span class=hljs-number >2</span>, next_action=<span class=hljs-number >2</span>)
get_trace(t, :state)
empty!(t)</code></pre> <figure class="l-page text-center"> <img src="/guide/trajectory.png"> <figcaption>A visual explanation of <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_core/#ReinforcementLearningCore.CircularCompactSARTSATrajectory"><code>CircularCompactSARTSATrajectory</code></a></figcaption> </figure> <h3 id=a_concrete_example ><a href="#a_concrete_example">A concrete example</a></h3> <p>The following code constructs an agent to use the <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_zoo/#ReinforcementLearningZoo.BasicDQNLearner"><code>BasicDQNLearner</code></a>. It is the same with the one used in the <code>JuliaRL_BasicDQN_CartPole</code> experiment.</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> ReinforcementLearning
<span class=hljs-keyword >using</span> Random
<span class=hljs-keyword >using</span> Flux
rng = MersenneTwister(<span class=hljs-number >123</span>)
env = CartPoleEnv(;T = <span class=hljs-built_in >Float32</span>, rng = rng)
ns, na = length(get_state(env)), length(get_actions(env))
agent = Agent(
    policy = QBasedPolicy(
        learner = BasicDQNLearner(
            approximator = NeuralNetworkApproximator(
                model = Chain(
                    Dense(ns, <span class=hljs-number >128</span>, relu; initW = glorot_uniform(rng)),
                    Dense(<span class=hljs-number >128</span>, <span class=hljs-number >128</span>, relu; initW = glorot_uniform(rng)),
                    Dense(<span class=hljs-number >128</span>, na; initW = glorot_uniform(rng)),
                ) |&gt; cpu,
                optimizer = ADAM(),
            ),
            batch_size = <span class=hljs-number >32</span>,
            min_replay_history = <span class=hljs-number >100</span>,
            loss_func = huber_loss,
            rng = rng,
        ),
        explorer = EpsilonGreedyExplorer(
            kind = :exp,
            ϵ_stable = <span class=hljs-number >0.01</span>,
            decay_steps = <span class=hljs-number >500</span>,
            rng = rng,
        ),
    ),
    trajectory = CircularCompactSARTSATrajectory(
        capacity = <span class=hljs-number >1000</span>,
        state_type = <span class=hljs-built_in >Float32</span>,
        state_size = (ns,),
    ),
)</code></pre> <aside>You might have noticed that <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> is used here to build the deep learning model. With the abstraction layer of <code>Approximator</code>, we can replace Flux.jl with Knet.jl or even PyTorch or TensorFlow.</aside> <p>In the construction part of <code>BasicDQNLearner</code>, a <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_core/#ReinforcementLearningCore.NeuralNetworkApproximator"><code>NeuralNetworkApproximator</code></a> is used to estimate the Q value. The core algorithm part is implemented in the learner. The <code>BasicDQNLearner</code> accepts an environment and returns state-action values. The resulted agent is shown below:</p> <pre><code class="plaintext hljs">UndefVarError: get_state not defined
</code></pre> <h2 id=how_to_write_a_customized_environment ><a href="#how_to_write_a_customized_environment">How to write a customized environment?</a></h2> <p>See the detailed <a href="/blog/how_to_write_a_customized_environment/">blog</a>.</p> <h2 id=how_to_write_a_customized_stop_condition ><a href="#how_to_write_a_customized_stop_condition">How to write a customized stop condition?</a></h2> <p>Stop condition is just a function which is executed after interacting environment and returns a bool value indicating whether to stop an experiment or not.</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> hook(agent, env)::<span class=hljs-built_in >Bool</span>
    <span class=hljs-comment ># ...</span>
<span class=hljs-keyword >end</span></code></pre> <p>Usually a closure or a functional object will be used to store some intermediate data.</p> <h2 id=how_to_write_a_customized_hook ><a href="#how_to_write_a_customized_hook">How to write a customized hook?</a></h2> <p>In most cases, you don&#39;t need to write a customized hook. Some ver general hooks are provided so that you can inject any runtime logic at appropriate time:</p> <ul> <li><p><a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_core/#ReinforcementLearningCore.DoEveryNStep"><code>DoEveryNStep</code></a></p> <li><p><a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_core/#ReinforcementLearningCore.DoEveryNEpisode"><code>DoEveryNEpisode</code></a></p> </ul> <p>However, if you do need to write a customized hook, the following methods must be provided:</p> <ul> <li><p><code>&#40;hook::YourHook&#41;&#40;::PreActStage, agent, env, action&#41;</code>, note that there&#39;s an extra argument of <code>action</code>.</p> <li><p><code>&#40;hook::YourHook&#41;&#40;::PostActStage, agent, env&#41;</code></p> <li><p><code>&#40;hook::YourHook&#41;&#40;::PreEpisodeStage, agent, env&#41;</code></p> <li><p><code>&#40;hook::YourHook&#41;&#40;::PostEpisodeStage, agent, env&#41;</code></p> </ul> <p>If your hook is a subtype of <code>AbstractHook</code>, then all the above methods will have a default implementation which just returns <code>nothing</code>. So that you only need to extend the necessary method you want.</p> <h2 id=how_to_use_tensorboard ><a href="#how_to_use_tensorboard">How to use TensorBoard?</a></h2> <p>This package adopts a non-invasive way for logging. So you can log everything you like with a hook. For example, to log the loss of each step. You can use the <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_core/#ReinforcementLearningCore.DoEveryNStep"><code>DoEveryNStep</code></a>.</p> <pre><code class="julia hljs">DoEveryNStep() <span class=hljs-keyword >do</span> t, agent, env
    with_logger(lg) <span class=hljs-keyword >do</span>
        <span class=hljs-meta >@info</span> <span class=hljs-string >&quot;training&quot;</span> loss = agent.policy.learner.loss
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span>,</code></pre> <h2 id=how_to_evaluate_an_agent_during_training ><a href="#how_to_evaluate_an_agent_during_training">How to evaluate an agent during training?</a></h2> <p>Well, just like the matryoshka doll, we run an experiment inside an experiment with a hook&#33;</p> <pre><code class="julia hljs">run(
    agent,
    env,
    stop_condition
    DoEveryNStep(EVALUATION_FREQ) <span class=hljs-keyword >do</span> t, agent, env
        Flux.testmode!(agent)
        run(agent, env, eval_stop_condition, eval_hook)
        Flux.trainmode!(agent)
    <span class=hljs-keyword >end</span>
    )</code></pre> <figure class="l-body text-center"> <img src="/guide/dolls.gif"> <figcaption>From https://cdn.dribbble.com/users/882503/screenshots/3744602/dolls.gif</figcaption> </figure> <div></div></d-article> <d-appendix> <d-bibliography src="/guide/bibliography.bib"></d-bibliography> </d-appendix> <div class="distill-site-nav distill-site-footer"> <div class=row > <div class=col-md-3 ></div> <div class=col-md-6 > <p>This website is built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> of the <a href="https://github.com/tlienart/DistillTemplate">DistillTemplate</a> (licensed under <a href="https://github.com/distillpub/template/blob/master/LICENSE">Apache License 2.0</a>) and <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a>. The <a href="https://github.com/JuliaReinforcementLearning/JuliaReinforcementLearning.github.io">source code</a> of this website is licensed under <a href="https://github.com/JuliaReinforcementLearning/JuliaReinforcementLearning.github.io/blob/master/LICENSE">MIT License</a>. The <a href="https://github.com/JuliaReinforcementLearning">JuliaReinforcementLearning</a> organization was first created by <a href="https://github.com/jbrea">Johanni Brea</a> and then co-maintained by <a href="https://github.com/findmyway">Jun Tian</a>. And we thank <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl#contributors-">all the contributors </a> .</p> </div> <div class=col-md-3 ></div> </div> </div>